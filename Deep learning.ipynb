{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Deep learning.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcd-h0csCpOe"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hpuwVkH4MJX"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "import os, shutil\n",
        "from os import path\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import zipfile\n",
        "from keras import models \n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd \n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set a random seed\n",
        "from numpy.random import seed\n",
        "seed(1)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxQ6ipNwDDwK"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjBaSySh_W6c"
      },
      "source": [
        "#Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ5gAhuh5FUd",
        "outputId": "bfe5937d-ee74-4c78-afdd-db895f4eef14"
      },
      "source": [
        "#Choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('~/data')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "#Auto-iterate using the query syntax\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1tQbRH3uO-mwTbViQTNdzs_4x-UnUkoSz' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  #Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: Leukimia_DS.zip, id: 1Jk8GEaHikkiM9agFEh8VQTUtWcRzpWOk\n",
            "downloading to /root/data/Leukimia_DS.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWiG4_vp5z2S"
      },
      "source": [
        "with zipfile.ZipFile('/root/data/Leukimia_DS.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/root/data')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44Llx0GdDNaT"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnY31jz3AD6X"
      },
      "source": [
        "#The data didn't come all in the same format, the \"test\" set didn't have labels, so we eliminated it and\n",
        "#created a new test set, and the validation set didn't come in folders seperated by class, so we used\n",
        "#the following code to put it on that format. And then we zipped the data, uploaded it to a drive and\n",
        "#started using that file\n",
        "\n",
        "#Since the original test set had to be deleted, we decided to create new splits, having about 10000 images\n",
        "#for train, 3000 for validation and 3000 for test\n",
        "\n",
        "#data= pd.read_csv('C-NMC_test_prelim_phase_data_labels.csv')\n",
        "\n",
        "#lista=list(range(1,len(data)+1)\n",
        "#data['Names']=lista\n",
        "#ALLdata=data[data['labels']==1]\n",
        "#HMEdata=data[data['labels']==0]\n",
        "\n",
        "#src = \"Desktop/C-NMC_Leukemia/validation_data/C-NMC_test_prelim_phase_data\"\n",
        "#dst = \"Desktop/Leukimia DS/Val/ALL\"\n",
        "\n",
        "#for j in range(len(ALLdata)):\n",
        "#    files = [i for i in os.listdir(src) if i.startswith(ALLdata['Names'].iloc[j].astype(str)) and path.isfile(path.join(src, i))]\n",
        "#    for f in files:\n",
        "#        shutil.copy(path.join(src, f), dst)\n",
        "\n",
        "#src = \"Desktop/C-NMC_Leukemia/validation_data/C-NMC_test_prelim_phase_data\"\n",
        "#dst = \"Desktop/Leukimia DS/Val/HEM\"\n",
        "\n",
        "#for j in range(len(HMEdata)):\n",
        "#    files = [i for i in os.listdir(src) if i.startswith(HMEdata['Names'].iloc[j].astype(str)) and path.isfile(path.join(src, i))]\n",
        "#    for f in files:\n",
        "#        shutil.copy(path.join(src, f), dst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-_0uOudXykr",
        "outputId": "98cc948a-cb82-4e1c-a558-f9a67386a969"
      },
      "source": [
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "X_data = []\n",
        "files = glob.glob (\"/root/data/Leukimia DS/Train/ALL/*.bmp\")\n",
        "for myFile in files:\n",
        "    image = cv2.imread(myFile)\n",
        "    print(image.shape)\n",
        "    X_data.append(image)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(450, 450, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_dFpSozX-Eu"
      },
      "source": [
        "X2_data = []\n",
        "files = glob.glob (\"/root/data/Leukimia DS/Train/HEM/*.bmp\")\n",
        "for myFile in files:\n",
        "    image = cv2.imread(myFile)\n",
        "    X2_data.append(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-IJMCRjYWzt"
      },
      "source": [
        "y = []\n",
        "for i in range(len(X_data)):\n",
        "  y.append(1)\n",
        "y2 = []\n",
        "for i in range(len(X2_data)):\n",
        "  y.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZUzTSo7DZoU"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bbl3h3wDsMa",
        "outputId": "e44094fa-f24a-405c-cd75-c34f1b31b833"
      },
      "source": [
        "train_set = train_datagen.flow_from_directory('/root/data/Leukimia DS/Train/', \n",
        "                                                 target_size = (75, 75),\n",
        "                                                 batch_size = 20,\n",
        "                                                 class_mode = 'binary',\n",
        "                                                )\n",
        "\n",
        "val_set = test_datagen.flow_from_directory('/root/data/Leukimia DS/Val/', \n",
        "                                            target_size = (75, 75),\n",
        "                                            batch_size = 20,\n",
        "                                            class_mode = 'binary',\n",
        "                                           shuffle = False)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/root/data/Leukimia DS/Test/', \n",
        "                                            target_size = (75, 75),\n",
        "                                            batch_size = 20,\n",
        "                                            class_mode = 'binary',\n",
        "                                           )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10661 images belonging to 2 classes.\n",
            "Found 3144 images belonging to 2 classes.\n",
            "Found 3553 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHIJpp7NDTUg"
      },
      "source": [
        "# Model Creation and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsj5fdZe4JOP"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(75,75,3)))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Conv2D(256, (3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Flatten())\n",
        "#model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "#model.add(layers.Dense(512, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001), activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WevBssSXC21"
      },
      "source": [
        "#model = models.load_model('/root/data/ReducedInputSizeModel.h5')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HheOplUw4JOV",
        "outputId": "597de157-63be-4272-f530-bc0f998d7c76"
      },
      "source": [
        "hist= model.fit(\n",
        "        train_set,\n",
        "        steps_per_epoch= 250,\n",
        "        epochs= 50,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=150,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 75s 293ms/step - loss: 0.5249 - acc: 0.7400 - val_loss: 0.5280 - val_acc: 0.7590\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 70s 282ms/step - loss: 0.4614 - acc: 0.8000 - val_loss: 0.5148 - val_acc: 0.7527\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 70s 280ms/step - loss: 0.4462 - acc: 0.8087 - val_loss: 0.5093 - val_acc: 0.7583\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 69s 278ms/step - loss: 0.4549 - acc: 0.8053 - val_loss: 0.4997 - val_acc: 0.7670\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 69s 278ms/step - loss: 0.4517 - acc: 0.8008 - val_loss: 0.5098 - val_acc: 0.7673\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 69s 277ms/step - loss: 0.4423 - acc: 0.8099 - val_loss: 0.5093 - val_acc: 0.7687\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 69s 278ms/step - loss: 0.4348 - acc: 0.8186 - val_loss: 0.4961 - val_acc: 0.7737\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 70s 279ms/step - loss: 0.4325 - acc: 0.8157 - val_loss: 0.4940 - val_acc: 0.7813\n",
            "Epoch 9/50\n",
            "239/250 [===========================>..] - ETA: 2s - loss: 0.4131 - acc: 0.8185"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF8NUwdkFFbf"
      },
      "source": [
        "# Model Assessment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XocTFY634JOW"
      },
      "source": [
        "acc = hist.history['acc']\n",
        "val_acc = hist.history['val_acc']\n",
        "loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kSiv_KMUFO0"
      },
      "source": [
        "#model.save('/root/data/ReducedInputSizeModel.h5')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uxovqcs4JOX",
        "outputId": "b5731dbd-77f6-4dd6-e0b3-ea169b4299e2"
      },
      "source": [
        "Y_pred = model.predict(val_set,63) # ceil(num_of_test_samples / batch_size)\n",
        "Y_pred = (Y_pred>0.5)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(val_set.classes, Y_pred))\n",
        "print('Classification Report')\n",
        "target_names = ['PlaceHolder', 'PlaceHolder']\n",
        "print(classification_report(test_set.classes, Y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "[[164  70]\n",
            " [  5 385]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      NORMAL       0.97      0.70      0.81       234\n",
            "   PNEUMONIA       0.85      0.99      0.91       390\n",
            "\n",
            "    accuracy                           0.88       624\n",
            "   macro avg       0.91      0.84      0.86       624\n",
            "weighted avg       0.89      0.88      0.87       624\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}